{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Autoencoders**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to build autoencoders using Keras.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Load and preprocess the MNIST dataset for training an autoencoder. \n",
    "\n",
    "- Construct a simple autoencoder model using the Keras functional API. \n",
    "\n",
    "- Train the autoencoder on the MNIST dataset. \n",
    "\n",
    "- Evaluate the performance of the trained autoencoder. \n",
    "\n",
    "- Fine-tune the autoencoder to improve its performance. \n",
    "\n",
    "- Use the autoencoder to denoise images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Instructions: \n",
    "\n",
    "#### Step 1: Data Preprocessing \n",
    "\n",
    "This exercise prepares the MNIST dataset for training by normalizing the pixel values and flattening the images. Normalization helps in faster convergence during training, and flattening is required because the input layer of our autoencoder expects a one-dimensional vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.16.2 in /opt/conda/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.13.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.1.5)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 18:22:18.747206: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 18:22:18.748317: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-12 18:22:18.761914: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-12 18:22:18.802881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-12 18:22:18.984724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-12 18:22:18.984802: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-12 18:22:19.045492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-12 18:22:27.363630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.datasets import mnist \n",
    "\n",
    "# Load the dataset \n",
    "(x_train, _), (x_test, _) = mnist.load_data() \n",
    "\n",
    "# Normalize the pixel values \n",
    "x_train = x_train.astype('float32') / 255. \n",
    "x_test = x_test.astype('float32') / 255. \n",
    "\n",
    "# Flatten the images \n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) \n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "- Use Keras to load the MNIST dataset. \n",
    "- Normalize the image pixel values to the range [0, 1]. \n",
    "- Flatten the 28x28 images to a 784-dimensional vector to reshape the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Building the Autoencoder Model \n",
    "\n",
    "This exercise involves building an autoencoder with an encoder that compresses the input to 32 dimensions and a decoder that reconstructs the input from these 32 dimensions. The model is compiled with the Adam optimizer and binary crossentropy loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,960</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │        \u001b[38;5;34m50,960\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,392</span> (411.69 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m105,392\u001b[0m (411.69 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,392</span> (411.69 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,392\u001b[0m (411.69 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "\n",
    "# Encoder \n",
    "input_layer = Input(shape=(784,)) \n",
    "encoded = Dense(64, activation='relu')(input_layer) \n",
    "\n",
    "# Bottleneck \n",
    "bottleneck = Dense(32, activation='relu')(encoded) \n",
    "\n",
    "# Decoder \n",
    "decoded = Dense(64, activation='relu')(bottleneck) \n",
    "output_layer = Dense(784, activation='sigmoid')(decoded) \n",
    "\n",
    "# Autoencoder model \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "\n",
    "# Compile the model \n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy') \n",
    "\n",
    "# Summary of the model \n",
    "autoencoder.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "**1. Define the Encoder:**\n",
    "- Create an input layer with 784 neurons. \n",
    "- Add a Dense layer with 64 neurons and ReLU activation. \n",
    "\n",
    "**2. Define the Bottleneck:**\n",
    "- Add a Dense layer with 32 neurons and ReLU activation. \n",
    "\n",
    "**3. Define the Decoder:**\n",
    "- Add a Dense layer with 64 neurons and ReLU activation. \n",
    "- Add an output layer with 784 neurons and sigmoid activation. \n",
    "\n",
    "**4. Compile the Model:**\n",
    "- Use the Adam optimizer and binary crossentropy loss.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Training the Autoencoder \n",
    "\n",
    "In this exercise, the autoencoder is trained to reconstruct the MNIST images. The training data is both the input and the target, as the autoencoder learns to map the input to itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 164ms/step - loss: 0.2595 - val_loss: 0.1792\n",
      "Epoch 2/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - loss: 0.1599 - val_loss: 0.1427\n",
      "Epoch 3/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 165ms/step - loss: 0.1363 - val_loss: 0.1282\n",
      "Epoch 4/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 157ms/step - loss: 0.1255 - val_loss: 0.1202\n",
      "Epoch 5/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 163ms/step - loss: 0.1184 - val_loss: 0.1139\n",
      "Epoch 6/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 164ms/step - loss: 0.1135 - val_loss: 0.1102\n",
      "Epoch 7/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 165ms/step - loss: 0.1107 - val_loss: 0.1078\n",
      "Epoch 8/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 164ms/step - loss: 0.1085 - val_loss: 0.1060\n",
      "Epoch 9/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 162ms/step - loss: 0.1067 - val_loss: 0.1043\n",
      "Epoch 10/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 167ms/step - loss: 0.1051 - val_loss: 0.1029\n",
      "Epoch 11/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 167ms/step - loss: 0.1037 - val_loss: 0.1018\n",
      "Epoch 12/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 158ms/step - loss: 0.1026 - val_loss: 0.1007\n",
      "Epoch 13/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 160ms/step - loss: 0.1016 - val_loss: 0.0998\n",
      "Epoch 14/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 180ms/step - loss: 0.1007 - val_loss: 0.0987\n",
      "Epoch 15/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 169ms/step - loss: 0.0997 - val_loss: 0.0980\n",
      "Epoch 16/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 164ms/step - loss: 0.0990 - val_loss: 0.0974\n",
      "Epoch 17/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 164ms/step - loss: 0.0984 - val_loss: 0.0968\n",
      "Epoch 18/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 165ms/step - loss: 0.0978 - val_loss: 0.0964\n",
      "Epoch 19/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 160ms/step - loss: 0.0974 - val_loss: 0.0959\n",
      "Epoch 20/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 148ms/step - loss: 0.0969 - val_loss: 0.0955\n",
      "Epoch 21/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 162ms/step - loss: 0.0965 - val_loss: 0.0953\n",
      "Epoch 22/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 166ms/step - loss: 0.0961 - val_loss: 0.0949\n",
      "Epoch 23/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 157ms/step - loss: 0.0957 - val_loss: 0.0945\n",
      "Epoch 24/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 157ms/step - loss: 0.0953 - val_loss: 0.0939\n",
      "Epoch 25/25\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 170ms/step - loss: 0.0950 - val_loss: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x726768f6e300>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(\n",
    "    x_train, x_train,  \n",
    "    epochs=25,  \n",
    "    batch_size=256,  \n",
    "    shuffle=True,  \n",
    "    validation_data=(x_test, x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "- Use the `fit` method to train the model on the training data. \n",
    "- Set the number of epochs to 25 and the batch size to 256.. \n",
    "- Use the test data for validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluating the Autoencoder \n",
    "\n",
    "This exercise evaluates the autoencoder by reconstructing the test images and comparing them to the original images. Visualization helps in understanding how well the autoencoder has learned to reconstruct the input data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.9.2\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:02\u001b[0m\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.8\n",
      "    Uninstalling matplotlib-3.10.8:\n",
      "      Successfully uninstalled matplotlib-3.10.8\n",
      "Successfully installed matplotlib-3.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib==3.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Predict the test data \n",
    "reconstructed = autoencoder.predict(x_test) \n",
    "\n",
    "# Visualize the results \n",
    "n = 10  # Number of digits to display \n",
    "plt.figure(figsize=(20, 4)) \n",
    "\n",
    "for i in range(n): \n",
    "    # Display original \n",
    "    ax = plt.subplot(2, n, i + 1) \n",
    "    plt.imshow(x_test[i].reshape(28, 28)) \n",
    "    plt.gray() \n",
    "    ax.get_xaxis().set_visible(False) \n",
    "    ax.get_yaxis().set_visible(False) \n",
    "\n",
    "    # Display reconstruction \n",
    "    ax = plt.subplot(2, n, i + 1 + n) \n",
    "    plt.imshow(reconstructed[i].reshape(28, 28)) \n",
    "    plt.gray() \n",
    "    ax.get_xaxis().set_visible(False) \n",
    "    ax.get_yaxis().set_visible(False) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "**1. Reconstruct Images:**\n",
    "- Use the autoencoder to predict the test data. \n",
    "- Compare the original test images with the reconstructed images. \n",
    "\n",
    "**2. Visualize the Results:**\n",
    "- Plot a few examples of original and reconstructed images side by side. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Fine-Tuning the Autoencoder \n",
    "\n",
    "Fine-tuning the autoencoder by unfreezing some layers can help in improving its performance. In this exercise, you unfreeze the last four layers and train the model again for a few more epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers of the autoencoder\n",
    "for layer in autoencoder.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: input_layer, Trainable = False\n",
      "Layer 1: dense, Trainable = False\n",
      "Layer 2: dense_1, Trainable = False\n",
      "Layer 3: dense_2, Trainable = False\n",
      "Layer 4: dense_3, Trainable = False\n"
     ]
    }
   ],
   "source": [
    "# Check trainable status of each layer\n",
    "for i, layer in enumerate(autoencoder.layers):\n",
    "    print(f\"Layer {i}: {layer.name}, Trainable = {layer.trainable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 177ms/step - loss: 0.0948 - val_loss: 0.0933\n",
      "Epoch 2/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 177ms/step - loss: 0.0943 - val_loss: 0.0931\n",
      "Epoch 3/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 170ms/step - loss: 0.0940 - val_loss: 0.0928\n",
      "Epoch 4/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 179ms/step - loss: 0.0938 - val_loss: 0.0926\n",
      "Epoch 5/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 152ms/step - loss: 0.0936 - val_loss: 0.0924\n",
      "Epoch 6/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 149ms/step - loss: 0.0935 - val_loss: 0.0925\n",
      "Epoch 7/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 156ms/step - loss: 0.0934 - val_loss: 0.0924\n",
      "Epoch 8/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 152ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 9/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 155ms/step - loss: 0.0932 - val_loss: 0.0924\n",
      "Epoch 10/10\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 163ms/step - loss: 0.0931 - val_loss: 0.0920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x726765cf0590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfreeze the top layers of the encoder\n",
    "for layer in autoencoder.layers[-4:]: \n",
    "    layer.trainable = True \n",
    "\n",
    "# Compile the model again\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy') \n",
    "\n",
    "# Train the model again\n",
    "autoencoder.fit(x_train, x_train,  \n",
    "                epochs=10,  \n",
    "                batch_size=256,  \n",
    "                shuffle=True,  \n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "**1. Freeze all the Encoder Layers:**\n",
    "- Freeze all the layers of the encoder.\n",
    "\n",
    "**2. Check the Status:**\n",
    "- Checking the trainable status of each layer. \n",
    "\n",
    "**3. Unfreeze the Encoder Layers:**\n",
    "- Unfreeze the last four layers of the encoder. \n",
    "\n",
    "**4. Compile and Train the Model:**\n",
    "- Recompile the model. \n",
    "- Train the model again for 10 epochs with the same training and validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Denoising Images with Autoencoder \n",
    "\n",
    "In this exercise, you add random noise to the dataset and train the autoencoder to denoise the images. The autoencoder learns to reconstruct the original images from the noisy input, which can be visualized by comparing the noisy, denoised, and original images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add noise to the data\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Train the autoencoder with noisy data\n",
    "autoencoder.fit(\n",
    "    x_train_noisy, x_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test_noisy, x_test)\n",
    ")\n",
    "\n",
    "# Denoise the test images\n",
    "reconstructed_noisy = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "# Visualize the results\n",
    "n = 10  # Number of digits to display\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Display noisy images\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # Display denoised images\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display original images\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "**1. Add noise to the data:**\n",
    "- Add random noise to the training and test data. \n",
    "- Train the Autoencoder with noisy data: \n",
    "-Train the autoencoder using the noisy images as input and the original images as target. \n",
    "\n",
    "**2. Evaluate the denoising performance:**\n",
    "- Use the autoencoder to denoise the test images. \n",
    "- Compare the noisy, denoised, and original images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    "### Exercise 1: Exploring Different Bottleneck Sizes \n",
    "\n",
    "#### Objective: \n",
    "\n",
    "To understand the impact of different bottleneck sizes on the performance of the autoencoder. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "**1. Define new models with different bottleneck sizes:**\n",
    "- Create three new autoencoder models, each with a different bottleneck size (e.g., 16, 32, and 64 neurons). \n",
    "- Use the same encoder and decoder architecture as in the main lab but change the number of neurons in the bottleneck layer. \n",
    "\n",
    "**2. Train the models:**\n",
    "- Train each model on the MNIST dataset for 50 epochs with a batch size of 256. \n",
    "- Use the same preprocessing steps as in the main lab. \n",
    "\n",
    "**3. Evaluate and Compare the Models:**\n",
    "- Evaluate the performance of each model on the test data. \n",
    "- Compare the reconstruction loss of the models to understand how the bottleneck size affects the autoencoder's ability to reconstruct the input data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define and train three different autoencoders with varying bottleneck sizes\n",
    "bottleneck_sizes = [16, 32, 64]\n",
    "autoencoders = []\n",
    "\n",
    "for size in bottleneck_sizes:\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(784,))\n",
    "    encoded = Dense(64, activation='relu')(input_layer)\n",
    "    bottleneck = Dense(size, activation='relu')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Dense(64, activation='relu')(bottleneck)\n",
    "    output_layer = Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    autoencoder.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        epochs=20,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, x_test)\n",
    "    )\n",
    "    autoencoders.append(autoencoder)\n",
    "\n",
    "# Evaluate and compare the models\n",
    "for i, size in enumerate(bottleneck_sizes):\n",
    "    loss = autoencoders[i].evaluate(x_test, x_test)\n",
    "    print(f'Bottleneck size {size} - Test loss: {loss}')\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Adding Regularization to the Autoencoder \n",
    " \n",
    "#### Objective: \n",
    "\n",
    "To explore the effect of regularization on the performance of the autoencoder. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "**1. Modify the model:**\n",
    "- Add L2 regularization to the Dense layers in both the encoder and decoder parts of the autoencoder. \n",
    "\n",
    "**2. Train the model:**\n",
    "- Train the modified autoencoder on the MNIST dataset for 50 epochs with a batch size of 256. \n",
    "\n",
    "**3. Evaluate and compare:**\n",
    "- Evaluate the performance of the regularized autoencoder and compare it with the non-regularized version. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "from tensorflow.keras.regularizers import l2 \n",
    "\n",
    "# Encoder with L2 regularization \n",
    "input_layer = Input(shape=(784,)) \n",
    "encoded = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(input_layer) \n",
    "bottleneck = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(encoded) \n",
    "\n",
    "# Decoder with L2 regularization \n",
    "decoded = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(bottleneck) \n",
    "output_layer = Dense(784, activation='sigmoid', kernel_regularizer=l2(0.01))(decoded) \n",
    "\n",
    "# Autoencoder model with L2 regularization \n",
    "autoencoder_regularized = Model(input_layer, output_layer) \n",
    "autoencoder_regularized.compile(optimizer='adam', loss='binary_crossentropy') \n",
    "\n",
    "# Train the model \n",
    "autoencoder_regularized.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test)) \n",
    "\n",
    "# Evaluate the model \n",
    "loss = autoencoder_regularized.evaluate(x_test, x_test) \n",
    "print(f'Regularized Autoencoder - Test loss: {loss}') \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Visualizing Intermediate Representations \n",
    "\n",
    "#### Objective: \n",
    "\n",
    "To visualize and understand the intermediate representations (encoded features) learned by the autoencoder. \n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "**1. Extract Encoder Part:**\n",
    "- Extract the encoder part of the trained autoencoder to create a separate model that outputs the encoded features. \n",
    "\n",
    "**2. Visualize Encoded Features:**\n",
    "- Use the encoder model to transform the test data into the encoded space. \n",
    "- Plot the encoded features using a scatter plot for the first two dimensions of the encoded space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writw your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Extract the encoder part of the autoencoder \n",
    "encoder_model = Model(input_layer, bottleneck) \n",
    "\n",
    "# Encode the test data \n",
    "encoded_imgs = encoder_model.predict(x_test) \n",
    "\n",
    "# Visualize the first two dimensions of the encoded features \n",
    "plt.figure(figsize=(10, 8)) \n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c='blue', alpha=0.5) \n",
    "plt.title('Encoded Features - First Two Dimensions') \n",
    "plt.xlabel('Encoded Feature 1') \n",
    "plt.ylabel('Encoded Feature 2') \n",
    "plt.show() \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "\n",
    "Congratulations on completing this lab! In this lab, you have gained practical experience in building, training, and evaluating autoencoders using Keras. You have learned to preprocess data, construct a basic autoencoder architecture, train the model on the MNIST dataset, and visualize the results. Additionally, you explored fine-tuning techniques to enhance the model's performance and applied the autoencoder to denoise images. \n",
    "\n",
    "Continue experimenting with different architectures, datasets, and applications to further deepen your knowledge and skills in using autoencoders. The concepts and techniques you have learned in this lab will serve as a foundation for more advanced topics in deep learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skills Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8b4582edd96284dd599c093b9302570ac4eaf6195d34c0cc6a30d9cd17694dff"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
